---
title: "Lab_10T"
author: "36-600"
date: "Fall 2022"
output: 
  html_document:
    toc: no
    toc_float: no
    theme: spacelab
---
```{r}
suppressMessages(library(dplyr))
suppressWarnings(library(tidyverse))
suppressWarnings(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(GGally))
suppressMessages(library(car))
suppressMessages(library(ggplot2))
suppressMessages(library(randomForest))
```

# Regression

We import the heart-disease dataset and log-transform the response variable, `Cost`:
```{r}
df      <- read.csv("http://www.stat.cmu.edu/~pfreeman/heart_disease.csv",stringsAsFactors=TRUE)
df      <- df[,-10]
w       <- which(df$Cost > 0)
df      <- df[w,]
df$Cost <- log(df$Cost)
```

## Question 1

Split these data into training and test sets, reusing the code you used in Lab 06R and Lab 09T.
```{r}
# FILL ME IN
set.seed(100)
s <- sample(nrow(df),round(0.7*nrow(df)))
df.train = df[s,]
df.test = df[-s,]
```

## Question 2

Learn a random forest model given the training data, and compute the MSE. Remember to set `importance=TRUE`. **Note: for reproducible results, set the seed before running random forest!** Assuming you split the data in the same manner as you did before, feel free to look back at your other labs and see if the MSE is smaller here. (For me and my split? It is...about 10% smaller than for a regression tree.)
```{r}
# FILL ME IN
rf.out = randomForest(df.train$Cost~.,data=df.train,importance=TRUE)
cost.pred = predict(rf.out,newdata=df.test)
MSE = mean((df.test$Cost-cost.pred)^2)
MSE
```

## Question 3

Create the variable importance plot. Remember to pass `type=1` as an argument to this plot. Mentally note the important variables. These should be consistent with those variables that appeared in your regression tree in the tree lab.
```{r}
# FILL ME IN
varImpPlot(rf.out,type=1)
```

## Question 4

Show the diagnostic plot of predicted test-set response values vs. observed test-set response values. As usual, make sure the limits are the same along both axes and plot a diagonal line with slope 1.
```{r}
# FILL ME IN
suppressMessages(library(tidyverse))
ggplot(data=data.frame("x"=df.test$Cost,"y"=cost.pred),mapping=aes(x=x,y=y)) + geom_point(size=0.4,color="black") + xlim(0,12) + ylim(0,12) + geom_abline(intercept=0,slope=1,color="red")
```

## Question 5

Now learn an extreme gradient boosting model, and show the test-set MSE. Note that in order to do this, we have to remove the variables `Gender`, `Drugs`, and `Complications`, which are factor or factor-like variables, and for ease of code implementation, we will break up `df.train` and `df.test` into predictor and response variables:
```{r}
df.train %>% dplyr::select(.,-Gender,-Drugs,-Complications) -> df.train
df.test  %>% dplyr::select(.,-Gender,-Drugs,-Complications) -> df.test
resp.train <- df.train[,1]
resp.test  <- df.test[,1]
pred.train <- df.train[,-1]
pred.test  <- df.test[,-1]
```
Note that by doing this, the MSE that we get might not be as good as for random forest. But we'll see!
```{r}
suppressMessages(library(xgboost))
```
```{r}
# FILL ME IN
df.train = xgb.DMatrix(data=as.matrix(pred.train),label=resp.train)
df.test  = xgb.DMatrix(data=as.matrix(pred.test),label=resp.test)
set.seed(101)
xgb.cv.out = xgb.cv(params=list(objective="reg:squarederror"),df.train,nrounds=30,nfold=5,verbose=0)
cat("Optimal number of trees is ",which.min(xgb.cv.out$evaluation_log$test_rmse_mean),"\n")
```
```{r}
xgb.out = xgboost(df.train,nrounds=which.min(xgb.cv.out$evaluation_log$test_rmse_mean),
                  params=list(objective="reg:squarederror"),verbose=0)
resp.pred = predict(xgb.out,newdata=df.test)
round(mean((resp.pred-resp.test)^2),3)
```

## Question 6

Create a variable importance plot for the extreme gradient boosting model. Make a mental note about whether the variables identified as important here are also the more important ones identified by random forest.
```{r}
# FILL ME IN
imp.out = xgb.importance(model=xgb.out)
xgb.plot.importance(importance_matrix=imp.out,col="black")
```

---

# Classification

We will now load in the data on political movements that you looked at in previous labs:
```{r}
file.path <- "http://www.stat.cmu.edu/~pfreeman/movement.Rdata"
load(url(file.path))
f <- function(variable,level0="NO",level1="YES") {
  n               <- length(variable)
  new.variable    <- rep(level0,n)
  w               <- which(variable==1)
  new.variable[w] <- level1
  return(factor(new.variable))
}
predictors$nonviol      <- f(predictors$nonviol)
predictors$sanctions    <- f(predictors$sanctions)
predictors$aid          <- f(predictors$aid)
predictors$support      <- f(predictors$support)
predictors$viol.repress <- f(predictors$viol.repress)
predictors$defect       <- f(predictors$defect)
levels(response)        <- c("FAILURE","SUCCESS")
rm(file.path,id.half,id,predictors.half)
```

Note that given the number of factor variables in this dataset, we'll forego learning a boosting model below.

## Question 7

Split the data! Feel free to recreate what you did for previous labs.
```{r}
# FILL ME IN
set.seed(100)
r_index = sample(c(TRUE,FALSE), nrow(predictors),replace=TRUE,prob=c(0.8,0.2))
pred.train = predictors[r_index,]
pred.test = predictors[!r_index,]
resp.train = response[r_index]
resp.test = response[!r_index]
```

## Question 8

Learn a random forest model. Output probabilities for Class 1 (see the notes!) but do not output a confusion matrix or output a misclassification rate. It will become clear why we will hold off on computing this quantities for now... However, having said all this, do go ahead and plot the variable importance plot here.
```{r}
# FILL ME IN
rf.out = randomForest(resp.train~.,data=pred.train,importance=TRUE)
resp.prob = predict(rf.out,newdata=pred.test)
resp.pred = ifelse(resp.prob > 0.5,"SUCCESS","FAILURE")
```

## Question 9
```{r}
suppressMessages(library(pROC))
```

Plot a ROC curve for random forest, and output the AUC value.
```{r}
# FILL ME IN
resp.prob = predict(rf.out,newdata=pred.test,type="prob")[,2]
roc.log = roc(resp.test,resp.prob)
plot(roc.log,col="black",xlim=c(1,0),ylim=c(0,1)) 
```
```{r}
cat("AUC for logistic regression: ",round(roc.log$auc,3))
```

## Question 10

Use Youden's $J$ statistic to determine the optimal class-separation threshold. Output that number. Then, using that threshold, transform the test-set Class 1 probabilities to class predictions, and output the confusion matrix and the misclassification rate. (Note: you can reuse code from Question 7 of Lab 09R, if you wish.)
```{r}
# FILL ME IN
rf.out = randomForest(resp.train~.,data=pred.train)
resp.prob = predict(rf.out,newdata=pred.test,type="prob")[,2]
resp.pred = ifelse(resp.prob>0.5,"SUCCESS","FAILURE")
round(mean(resp.pred!=resp.test),3)
```
```{r}
table(resp.pred,resp.test)
```

